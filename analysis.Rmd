---
output: html_document
---

# Regression Analysis

Now that weâ€™ve properly conducted an EDA of our federal sentencing data and have an understanding of the context and the structure of the data, we can dive into analyzing this data further! 

One main way that we analyze data is through fitting regressions. Regressions can be used to model the relationships between different explanatory variables and a chosen response variable. There are many kinds of regressions, but the most common and simplest regression is a simple linear regression. To explore linear regression, let's first take a step back from the federal sentencing data, and turn to two simpler data sets for examples.  

## Fitting a line: The basics

Suppose we want to predict how much electricity the city of Los Angeles, California will use based on the daily temperature. As the temperature goes higher, more people will turn on their air conditioners and use more electricity. We can look at past electricity use data and compare it to the temperature each day to get a sense of how these two attributes are related. Knowing and understanding how two variables relate can help you plan for future possibilities or identify and correct patterns that you don't want to continue. For example, we can use this relationship to makes predictions of how much electricity Los Angeles will use in the future if we know the future temperature, and make sure that there is enough for the city's needs.

Let's make two example points, point A at (1,2) and point B at (3,5). In R, we will save these points in a data frame using a vector of the x values, 1 and 3, and a vector of the matching y values, 2 and 5.

```{r}
twopoints <- data.frame(xvals = c(1, 3), 
                        yvals = c(2, 5), 
                        label = c("A","B"))
head(twopoints)
```

We can make a fairly simple plot of these two points, using `geom_point()` as we did in our EDA, and `geom_text()` to label our points. 

```{r twopoints}
twoplot <- ggplot(twopoints, aes(x = xvals, y = yvals)) + 
  geom_point(color = "coral2") + 
  geom_text(aes(label = label), nudge_y = 0.3) +
  xlim(0, 6) + 
  ylim(0, 6)

twoplot
```

### Analytically fit a line to two points

In order to create a linear regression on these two points, we can think back to algebra and use the formula for a line.

$$ 
y = m x + b
$$

Fitting our line to the data is straightforward and can be done in a number of ways. One way is that we can plug both points into the equation of the line and then solve the system together.

$$
\begin{align}
2 &= (1)m + b\\
5 &= (3)m + b
\end{align}
$$

Here we have a system that has two equations and two unknowns ($m$ and $b$), so we know this system has a unique solution! We can solve this system using a variety of techniques---try to solve this system using a technique you are comfortable with and verify that the solution below passes through each of the two points.

$$
y = \frac{3}{2} x + \frac{1}{2}
$$

Next we can plot the results. Here we use the `geom_abline()` function, to plot our linear equation which can be done by inputting the values for the slope and the intercept.

```{r twopoints lm}
twoplot + 
  geom_abline(slope = 3/2, 
              intercept = 1/2)
```

Great! Notice that the line above goes right through our two data points.

### Numerically fit a line to two points

If we want to reduce the amount of calculations required to fit a line to two points, we can instead just rely on R to do the work for us. We can use the linear model function `lm()` to carry out a regression for us. We just input our `yvals` and `xvals` along with our data `twopoints` to fit a linear model using R. 

```{r}
two_lm <- lm(yvals ~ xvals, data = twopoints)
two_lm
```

### Analytically fit a line to three points

We know that two points alone uniquely define a line, but what do we think will happen if we have to find a line that goes through three data points? Let's add the point $(2, 3)$ to our existing set and see what happens when try to draw a line through all three points. Below, we will use R to plot the three points and make multiple attempts to fit a line. Don't worry too much on the coding for now, but pay attention to the resulting plots. 

```{r threepoints line guesses, fig.width=12}

threepoints <- 
  twopoints %>%
  # Add third point to dataset
  add_row(xvals = 2, yvals = 3, label = "C") %>%
  # Generate y values for guesses of regression lines 
  mutate(yfit1 = xvals * 3/2 + 1/2,
         yfit2 = xvals + 1,
         yfit3 = xvals * 2 - 1)

# Plot data points with lines
threeplot <-
  ggplot(threepoints, aes(x = xvals, y = yvals)) + 
  geom_point(color = "coral2")  + 
  geom_text(aes(label = label), 
            nudge_y = 0.3, 
            check_overlap = TRUE) +
  xlim(0,6) + 
  ylim(0,6)

# Display plots
grid.arrange(
  threeplot + 
    geom_abline(slope = 3/2, intercept = 1/2) + 
    geom_segment(aes(xend = xvals, 
                     yend = yfit1), 
                 color = "coral2"),
  threeplot + 
    geom_abline(slope = 1, intercept = 1) + 
    geom_segment(aes(xend = xvals, 
                     yend = yfit2), 
                 color = "coral2"),
  threeplot + 
    geom_abline(slope = 2, intercept = -1) +  
    geom_segment(aes(xend = xvals, 
                     yend = yfit3), 
                 color="coral2"),
  ncol = 3)
```

Notice in all three graphs above, we can't draw a straight line through all three points at the same time. The best that we can do is try to find a line that gets very close to all three points, or fits these three points *the best*. But how can we define "the best" line that fits this data?

To understand which line *best fits* our three data points, we need to talk about the **error**, which is also called the **residual** in statistics. The residual is the vertical distance between the predicted data point $\hat{y}$ (on the line) and the actual value of $y$ our data takes on at that point (the value we collected). In our data set we have the points $(1, 2)$, $(2, 3)$, and $(3, 5)$ so the only actual values for $y$ in our data set are 2, 3, and 5 even though our predicted line, or *linear model*, takes on all values of $y$ between 0 and 6.


### Numerically fit a line to three points

To find the model that best fits our data, we want to make the total error as small as possible. To expand on our definition of linear regression above, linear regression is a technique that allows us to identify the line that minimizes our error. This line is called a *linear regression model* and is the line that best fits our data. Below, we use `lm()` again to fit our linear model to the three data points.

```{r}
three_lm <- lm(yvals ~ xvals, data = threepoints)
three_lm
```

Here R gives us the slope and intercept of the straight line that best fits our data. Let's graph this line together with our data using the code below. 

```{r three points lm}
# Add predicted values (yhat) to data
threepoints <-
  threepoints %>%
  mutate(yhat = 1.5 * xvals + 0.3333)

ggplot(threepoints, aes(x = xvals, y = yvals)) + 
  geom_point(color = "coral2")  + 
  geom_text(aes(label = label), 
            nudge_y = -0.4 ) +
  xlim(0, 6) + 
  ylim(0, 6) + 
  geom_abline(slope = 1.5, intercept = 0.3333) + 
  geom_segment(aes(xend = xvals, 
                   yend = yhat), 
               color = "coral2")
```

Notice that the best fit linear model doesn't go through any of our three points! **Why do you think that is?**

Keep in mind, our goal is to minimize our error as much as we can. In each of the four previous graphs, the error (or the distance between the predicted y value and the actual y value) is shown on the graph, highlighted in the coral color. Of the four plots we just made of lines through our three data points, which one seems to have the smallest error?

> Note: Whenever you employ linear regression, there are various conditions you must make sure are satisfied before proceeding. You can use the acronym L.I.N.E to remember that your residuals must satisfy **L**inearity, **I**ndependence, **N**ormality, and **E**qual variance conditions. For the purposes of this tutorial, we won't go in depth about what these conditions mean and how you can check these conditions, but make sure to keep them in mind for your future data analyses. Conditions are super important in statistical analysis!


## Introduction to linear regression

Now let's turn to another example data set to explore linear regression further. This new data set focuses on penguins. Do you think a linear model might be a good way to model the data? Run the code below load the new dataset and create a scatterplot showing the relationship between flipper length and body mass among penguins. 

```{r penguin scatterplot, warning = FALSE, fig.width = 5, fig.height = 4}
data(penguins)

pengplot <- ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + 
  geom_point(color = "coral2", alpha = 0.7) +
  labs(x = "Body Mass (grams)",
       y = "Flipper Length (mm)",
       title = "Flipper Length by Body Mass of Penguins")

pengplot
```

Take a look at the scatterplot: does it look like most of the data fall along a straight line? If the general shape is a line, then yes, we could try to model this data with linear regression.

```{r penguins lm, warning = FALSE}
pengfit <- lm(flipper_length_mm ~ body_mass_g, data = penguins)
pengfit

pengplot + 
  geom_abline(slope = 0.0152, 
              intercept = 137.0396)
```

### Summarizing a linear regression model

After we fit a linear regression model, we are often interested in summarizing the model and assessing how good the model fits the data. One function we can use for this is `summary()`. This function provides an overview of the fitted model.

```{r}
summary(pengfit)
```

There is a lot of information that comes out of this function, but for now we will focus on the coefficients and the $R^2$ value.

#### Interpreting coefficients for numerical predictors

In the middle of the summary output, the estimated coefficients are provided in the first column of the table. The intercept is $b = 136.7$ and the slope is $m = 0.015$. 

While the **intercept** typically represents the average outcome when all predictors are 0, it doesn't make sense in this case to talk about a penguin with a mass of 0 grams (impossible!), so we will not interpret the intercept.

The **slope** represents the change in the average outcome for a one unit increase in the value of the predictor. In this case, we would say that a one-gram increase in the mass of a penguin is associated with 0.015 mm longer flipper, *on average*.

#### Assessing model fit with $R^2$

Now focus on the bottom of the summary output and find the "Multiple R-squared" value. You probably found $R^2 = 0.759$ from the table. 

$R^2$ represents the proportion of variation in the response variable that is explained by the explanatory variable(s) in your linear model. In our case,  we found that 75.9% of the variation in flipper length can be explained by the body mass of penguins. That is a pretty high $R^2$, especially for observational data! Therefore, we can already consider this model to be quite good for modeling flipper length. 

<!--
Apart from looking at the $R^2$, it is a good idea to look at the resulting $t$-tests and $F$-tests. While t-tests looks at the significance of individual variables by comparing population means, F-tests examine the significance of overall models by comparing population variances. You can see the "t value" column for the test statistics from the t-tests, but you can mainly just focus on the resulting p-values in the "Pr(>|t|)" column. In this case, the p-value for `body_mass_g` is quite low, meaning it is a significant variable in this model predicting flipper length. At the bottom, you can see the F-statistic and associated p-value for the overall model. Again, the p-value is quite low, meaning the model is significant. These two outcomes of the t-test and F-test can increase our confidence in the model even further. 
-->

### Linear regression with a categorical predictor

So far we've only explored linear regression with numerical variables, but we can certainly use categorical variables in our model as well. Continuing with our penguins data, let's compare flipper length to penguin species. Remember that flipper length is a numerical variable and species is a categorical variable (with three levels: Adelie, Chinstrap, and Gentoo).

#### Binary categorical predictor 

To start, let's consider one level at a time, so we can get a good sense of the relationship between species and flipper length. Below we examine Adelie penguins first. We start off by using `mutate()` again to label our species as either "Adelie" or "Not Adelie". Then, we create a side-by-side boxplot using `geom_boxplot()` again. 

```{r, warning = FALSE}
pengAdelie <- penguins %>% 
  mutate(species = case_when(species != "Adelie" ~ "Not Adelie",
                            TRUE ~ "Adelie")) %>% 
  drop_na()

adelieplot <- ggplot(pengAdelie, aes(x = species,
                                     y = flipper_length_mm)) + 
  geom_boxplot(color = 'coral2', 
               alpha = 0.7) +
  labs(x = "Species",
       y = "Flipper Length (mm)",
       title = "Flipper Length by Adelie and non-Adelie Species")
adelieplot
```

We can see that Adelie penguins seem to have a shorter flipper length, on average, than non-Adelie penguins. 

Next, we create a linear model for the relationship between flipper length and whether a penguin is an Adelie penguin or not.

```{r}
amodel <- lm(flipper_length_mm ~ body_mass_g + species, 
             data = pengAdelie)
amodel
```
Do you think that our linear model is a good representation of the data? Let's turn to `summary()` again to answer this question.

```{r}
summary(amodel)
```

We can analyze this summary table like normal to determine whether this is a good model of our data. 

Let's assume for the purposes of this case study that we checked conditions and feel ready to interpret our linear model. For interpreting the coefficient of the categorical variable `species`, however, you may notice that the variable shows up a little differently than you might expect, being named `speciesNot Adelie` rather than just `species`. When R is dealing with categorical variables in linear models, it chooses one level to be a baseline, coded as 0, and makes the other level 1. Since `Not Adelie` is in the name of the categorical variable in the linear model, we know R decided to code this level as 1, and therefore `Adelie` as 0. This is exactly what we would expect because, unless we specify which level we want to be our baseline variable, R will just choose the level that is first in the alphabet to be the baseline. How should we interpret this categorical variable you may ask? It may help to write out the equation for the linear model:

$\widehat{FlipperLength} = 144.3 + 0.01237(BodyMass) + 8.311(speciesNotAdelie)$

Now, since we know we will enter 0 for `speciesNot Adelie` for Adelie penguins, and 1 for `speciesNot Adelie` for non-Adelie penguins, we can now make two separate equations, one for each category.

Plugging in 0 for `speciesNot Adelie`, we get the equation for Adelie penguins: 

$\widehat{FlipperLength} =  144.3 + 0.01237(BodyMass)$

Plugging in 1 for `speciesNot Adelie`, we get the equation for non-Adelie penguins: 

$\widehat{FlipperLength} =  152.61 + 0.01237(BodyMass)$

Awesome. What do you now see about the impact of the `species` variable on predicting flipper length? The only difference in the two equations is in the y-intercept in the model, meaning that flipper length for non-Adelie penguins is, on average, 8.311 mm longer than flipper length for Adelie penguins, across all body mass values. 

#### Categorical predictor with more than 2 levels

<!-- Add content -->

<!-- Regression line with boxplot

Let's plot this best fit linear model against with our scatterplot to compare. 

As we saw before, things look a little bit different when we are dealing with categorical variables. In these types of scatterplots, we would expect the best fit model to pass through the mean values of each level of our categorical variable (or each 'chunk' of data). See below.

```{r, warning = FALSE, eval = FALSE}
#coefs <- coef(lm(flipper_length_mm ~ species, data = pengAdelie))

ggplot(pengAdelie, aes(y = flipper_length_mm, 
                       x = body_mass_g, 
                       color = species)) +
  geom_smooth(method = "lm", se = FALSE)
```

Now we do the same thing for the other two species (Chinstrap and Gentoo) and plot all three graphs side by side.

```{r, fig.width=12, eval = FALSE}
penguins$isChinstrap = ifelse(penguins$species=='Chinstrap',1,0)

pengChinstrap <- penguins %>% 
  mutate(species = case_when(species != "Chinstrap" ~ "Not Chinstrap",
                            TRUE ~ "Chinstrap"))

chinstrapplot <- ggplot(pengChinstrap, aes(x = species,
                                     y = flipper_length_mm)) + 
  geom_boxplot(color = 'coral2', 
               alpha = 0.7) +
  labs(x = "Species",
       y = "Flipper Length (mm)")
chinstrapplot


penguins$isGentoo = ifelse(penguins$species=='Gentoo',1,0)

pengGentoo <- penguins %>% 
  mutate(species = case_when(species != "Gentoo" ~ "Not Gentoo",
                            TRUE ~ "Gentoo"))

gentooplot <- ggplot(pengGentoo, aes(x = species,
                                        y = flipper_length_mm)) + 
  geom_boxplot(color = 'coral2', 
               alpha = 0.7) +
  labs(x = "Species",
       y = "Flipper Length (mm)")
gentooplot

amodel = lm(formula = flipper_length_mm~isChinstrap, data=penguins)
cmodel = lm(formula = flipper_length_mm~isChinstrap, data=penguins)
gmodel = lm(formula = flipper_length_mm~isGentoo, data=penguins)

speciesbase = ggplot(penguins, aes(y=flipper_length_mm)) 
aplot = speciesbase + 
  geom_point(aes(x=isAdelie), color='coral2', alpha=0.7) + 
  geom_abline(slope=amodel$coefficients[2], intercept=amodel$coefficients[1])
cplot = speciesbase + 
  geom_point(aes(x=isChinstrap), color='coral2', alpha=0.7) + 
  geom_abline(slope=cmodel$coefficients[2], intercept=cmodel$coefficients[1])
gplot = speciesbase + 
  geom_point(aes(x=isGentoo), color='coral2', alpha=0.7) + 
  geom_abline(slope=gmodel$coefficients[2], intercept=gmodel$coefficients[1])

grid.arrange(aplot, cplot, gplot, ncol = 3, nrow)
```
What would happen if instead of creating linear models for each level of the categorical variable separately, we created a single linear model for flipper length versus all species types? The code below allows us to find that model in just one step.

```{r}
linmodel <- lm(flipper_length_mm ~ species, data = penguins)
linmodel
```

This model will look like $$Flipper Length = 190.103 + 5.72*(Chinstrap) + 27.133*(Gentoo)$$ where the variable Chinstrap is an indicator variable that takes on the value of 1 when the penguin is the species Chinstrap and takes on the value of 0 when the penguin is any other species. Similarly, the variable Gentoo is an indicator variable that takes on the value of 1 when the penguin is the species Gentoo and takes on the value of 0 when the penguin is any other species.

So...what happened to the Adelie penguins in our model?!?! Notice the variable for Adelie is missing in our model. It actually got absorbed by the y-intercept! We only need n-1 variables to represent n levels of a categorical variable because we have this y-intercept. When a penguin is of the species Adelie and we apply this linear model, the values for Gentoo and Chinstrap are both equal to 0, since the penguin is an Adelie. Thus our model would predict a flipper length of 190.103 for all Adelie penguins.

What is the flipper length predicted by our model for a Chinstrap penguin? What about a Gentoo penguin?

```{r, eval = FALSE}
peng_encoded = penguins %>% mutate(value = 1) %>% spread(species, value, fill = 0 )
head(peng_encoded)
```

-->

### Multiple linear regression

<!-- Add content -->


## Linear Regression with the Federal Criminal Sentencing Data

Now that we've explored the basics of linear regression using both numerical and categorical variables, let's return to the goal of this project: to explore patterns in federal criminal sentencing data from 2006-2020. We will show how to create relevant linear models for this case study, and discuss the results of these models in the following section. We won't dive into the specifics of why, based on the EDA, we will include certain variables over others in the linear regression. Rather, we will simply replicate the results of the paper mentioned at the beginning of this case study. For more information on how the researchers of that paper arrived their final models, please read the paper.

### Baseline Model

Let's try a model of `sentence_length` based on the variable `race`, using the same methods we used above.

```{r}
baselinemod <- lm(sentence_length ~ race,
                  data = us_sent)
summary(baselinemod)
```

Already, we can use some of the methods we used above to determine whether this is a good model for `sentence_length`. Considering the $R^2$, t-test, and F-test values, is this a good model for our data? 

You may have noticed that the Multiple $R^2$ is relatively low, indicating that our model using `race` accounts for only 1.5% of the variation in `sentence_length`. You also may have noticed that all variables in our model have low p-values from t-tests, meaning the variables are significant predictors of `race`, and the overall F-test has a low-p-value as well. Do you think we can make a better model using more variables to predict `sentence_length`? 

Here is one more tool you can use to determine the effectiveness of certain predictors in a linear model. You may recall that confidence intervals give us a reasonable margin of error for estimating the coefficients in front of predictors in a model. You can get the confidence intervals for the different levels of `race` from our baseline model using the following code:

```{r}
confint(baselinemod)
```

Here, you automatically get 95% confidence intervals. We can see that the estimates for the coefficients in our linear model are 18.51 for `raceblack`, 5.26 for `racehispanic` and -8.98 for `raceARI`, but that the 95% confidence interval for `raceblack` is 18.06 to 18.97, the 95% confidence interval for `racehispanic` is 4.73 to 5.80, and the 95% confidence interval for `raceARI` is -9.88 to -8.08. 

> Note: Remember that for interpreting these values, the actual value of the coefficient in the model has nothing to do with the strength of the predictor. For example, just because the estimated coefficient for `racehispanic` is half the value of the estimated coefficient for `raceblack`, that doesn't necessarily mean one predictor is between than the other. Rather, it is important to focus on the strength of these predictors by relying on statistical tests like the t-test. Also, remember that to interpret a 95% confidence interval, you should interpret the interval as follows: "If I were to repeatedly sample from this population many times, I would expect the true coefficient to fall within this interval 95% of the time". 


### District I Model
Alright, let's try to improve our model by adding some more predictors. The first model based on the resesarch paper this case study is based on includes defendant demographics, sentencing year, whether or not there was a guilty plea, relevant cell on the U.S. sentencing grid, whether there was a mandatory minimum sentence, presence of government-sponsored downward departures, and the interaction of judicial district and defendant race. An interaction term is employed when the one variable impacts another variable in some way. For our data set, this includes the following variables: `age`, `sex`, and `educ`, `year`, `guilty_plea`, `grid_cell`, `mandatory_min`, `gov_departures`, and the interaction of `district` and `race`. In this model, all terms except for the interaction derive from nationwide data. For example, this model assumes that all districts, on average, apply the U.S. sentencing grid in the same way. The code for formulating this linear regression model would follow the same format as the other linear models we've made:

```{r}
lm1 <- lm(sentence_length ~ age + sex + educ + year + mandatory_min + guilty_plea + grid_cell +
            gov_departures + district + race,
          data = us_sent,
          model = TRUE,
          y = TRUE)
```

We are omitting the linear model summary from this document because, as you can imagine, it becomes quite long with so many categorical variables in the model. However, you can take a look at the $R^2$ by using a dollar sign and the typing `r.squared` as follows:

```{r}
summary(lm1)$r.squared
```

That's a much higher $R^2$! Obviously, you aren't able to immediately see other factors that determine how effective this model is at predicting sentence lengths because of the ommitted linear model summary. However, you can already see that this model accounts for a lot more of the variability in `sentence_length` than our baseline model.

### District II Model

In the paper this case study is based on, the authors apply this model (along with alternate models) across all districts in the data set. In doing this, they were able to compare the disparity in sentences across different races. Let's try employing this model for just one district at a time. Let's start with Arkansas. To only look at the datapoints from Arkansas, we will start with using the `filter()` function. Then, we will fit another model identical to the first linear model, only without `district` as a predictor:

```{r}
us_sent_az <- us_sent %>%
  filter(district == "Arizona")

lm2 <- lm(sentence_length ~ age + sex + educ + year + mandatory_min + guilty_plea + grid_cell +
            gov_departures + race,
          data = us_sent_az,
          model = TRUE, 
          y = TRUE)
```

Let's extract the resulting $R^2$ again to get a sense for how well this model using only data from Arizona predicts sentence length:

```{r}
summary(lm2)$r.squared
```

In the paper, the authors cycle through all districts. While we won't go into how they accomplished this in this case study, we encourage you to look at some other districts of interest to you.

Here is one figure from their paper comparing the minoritized race - white disparity in sentencing using a 95% confidence interval:

*Figure 1: Minoritized Race-White Disparity in Sentence Length (Month) Across U.S. Districts Represented by District I and II models:*

![districtIIfigure1.](photos/fedsent_fig1.jpg)

Here is another figure from the paper displaying where these highlighted districts are located across the United States:

![districtIIfigure2.](photos/fedsent_fig2.jpg)
