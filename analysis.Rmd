---
output: html_document
---

# Analysis

Now that we've properly conducted an EDA of our federal sentencing data, we can dive into analyzing this data further. One main way that we analyze data is through fitting regressions. Regressions can be used to model the relationships between different explanatory variables and a chosen response variable. There are many kinds of regressions, but the most common and simplest regression is a simple linear regression. To explore linear regression, let's first take a step back from the federal sentencing data, and turn to two simpler data sets for examples. 

## Fitting Lines to Data

Suppose we want to predict how much electricity the city of Los Angeles, California will use based on the daily temperature. As the temperature goes higher, more people will turn on their air conditioners and use more electricity. We can look at past electricity use data and compare it to the temperature each day to get a sense of how these two attributes are related. Knowing and understanding how two variables relate can help you plan for future possibilities or identify and correct patterns that you don't want to continue. For example, we can use this relationship to makes predictions of how much electricity Los Angeles will use in the future if we know the future temperature, and make sure that there is enough for the city's needs.

Let's make two example points, point A at (1,2) and point B at (3,5). In R, we will save these points in a data frame using a vector of the x values, 1 and 3, and a vector of the matching y values, 2 and 5.

```{r}
twopoints <- data.frame(xvals = c(1,3), 
                        yvals = c(2,5), 
                        label = c('A','B'))
head(twopoints)
```

We can make a fairly simple plot of these two points, using `geom_point()` as we did in our EDA, and `geom_text()` to label our points. 

```{r}
twoplot <- ggplot(twopoints, aes(x = xvals, y = yvals)) + 
  geom_point(color = 'coral2') + 
  geom_text(aes(label = label), nudge_y = .3) +
  xlim(0, 6) + 
  ylim(0, 6)
twoplot
```

### Analytically Fit a Line to Two Points

In order to create a linear regression on these two points, we can think back to algebra and use the formula for a line.

$$ 
y = m x + b
$$

Fitting our line to the data is straightforward, we can solve in a number of ways. One way, is that we can plug both these points into the equation of the line and then solve the system together.

$$
2 = (1)m + b 
$$ $$5 = (3)m+b$$Here we have a system that has two equations and two unknowns, $m$ and $b$. We know this system has a unique solution! Since we can solve this system using a variety of techniques, try to solve this system using a technique you are comfortable with and verify that the solution below passes through each of the two points.

$$
y = \frac{3}{2} x+ \frac{1}{2}
$$

We can plot the results. Here we use the `geom_abline()` function, to plot our linear equation which can be done by inputting the values for the slope and the intercept.

```{r}
twoplot + geom_abline(slope = 3/2, 
                      intercept = 1/2)
```

Great, notice that the line above goes right through our two data points.

### Numerically Fit a Line to Two Points

If we want to reduce the amount of calculations required to fit a line to two points, we can instead just rely on R to do the work for us. We can use the linear model function, `lm()` to carry out a regression for us. We just input our `yvals` and `xvals` along with our data `twopoints` to fit a linear model using R. 

```{r}
twolinear <- lm(formula = yvals ~ xvals, 
                data = twopoints)
twolinear
```

### Analytically Fit a Line to Three Points

We know that two points alone uniquely define a line, but what do we think will happen if we have to find a line that describes the goes through three data points? Let's add the point (2,3) to our existing set and see what happens when try to draw a line through these three points. Below, we will use R to plot three graphs of our points, each attempting to find a line that goes through all three data points. Don't worry too much on the coding for now, but pay attention to the resulting plots. 

```{r, fig.width=12}
threepoints = rbind(twopoints, 
                    data.frame(xvals = 2, yvals = 3, label = 'C'))
threepoints$yfit1 = threepoints$xvals*3/2 + 1/2
threepoints$yfit2 = threepoints$xvals + 1
threepoints$yfit3 = threepoints$xvals*2 - 1

threeplot = ggplot(threepoints, aes(x = xvals, 
                                    y = yvals)) + 
  geom_point(color = 'coral2')  + 
  geom_text(aes(label = label), 
            nudge_y = 0.3, 
            check_overlap = TRUE) +
  xlim(0,6) + 
  ylim(0,6)

grid.arrange(
  threeplot + 
    geom_abline(slope = 3/2, intercept = 1/2) + 
    geom_segment(aes(xend = xvals, 
                     yend = yfit1), 
                 color = 'coral2'),
  threeplot + 
    geom_abline(slope = 1, intercept = 1) + 
    geom_segment(aes(xend = xvals, 
                     yend = yfit2), 
                 color = 'coral2'),
  threeplot + 
    geom_abline(slope = 2, intercept = -1) +  
    geom_segment(aes(xend = xvals, 
                     yend = yfit3), 
                 color='coral2'),
  ncol=3)
```

Notice in all three graphs above, we can't draw a straight line through all three points at the same time. The best that we can do is try to find a line that gets very close to all theree points, or fits these three points the best. But how can we define "the best" line that fits this data?

To understand which line *best fits* our three data points, we need to talk about the **error**, which is also called the **residual** in statistics. The residual is the vertical distance between the predicted data point y (on the line) and the actual value of y our data takes on at that  point (the value we collected) at each of our data points. In our data set we have the points (1,2), (2,3), and (3,5) so the only actual values for y in our data set are 2, 3,and 5 even though our prediction line (our model) takes on all values of y between 0 and 6.


### Numerically Fit a Line to Three Points
To find the model that best fits our data, we want to make the error as small as possible. To expand on our definition of linear regression above, linear regression is a technique that allows us to identify the line that minimizes our error. This line is called a *linear regression model* and is the line that best fits our data. Below, you will see R code to identify the model that best fits our data.

```{r}
threelinear = lm(formula = yvals ~ xvals, 
                 data = threepoints)
threelinear
```

```{r}
threepoints$linfit = 1.5*threepoints$xvals + 0.3333
ggplot(threepoints, aes(x = xvals, 
                        y = yvals)) + 
  geom_point(color = 'coral2')  + 
  geom_text(aes(label = label), 
            nudge_y = -0.4 ) +
  xlim(0,6) + 
  ylim(0,6) + 
  geom_abline(slope = 1.5, intercept = 0.3333) + 
  geom_segment(aes(xend = xvals, 
                   yend = linfit), 
               color = 'coral2')
```

Notice that the best fit linear model doesn't go through any of our three points! **Why do you think that is?**

Keep in mind, our goal is to minimize our error as much as we can. In each of the four previous graphs, the error (or the distance between the predicted y value and the actual y value) is shown on the graph, highlighted in the coral color. Of the four plots we just made of lines through our three data points, which one seems to have the smallest error?

> NOTE: Whenever you employ linear regression, there are various conditions you must make sure are satisfied before proceeding. You can use the acronym L.I.N.E to remember that you must satisfy linearity, independence, normality, and equal variance conditions. For the purposes of this tutorial, we won't go in depth about what these conditions mean and how you can check these conditions, but make sure to keep them in mind for your future data analyses. Conditions are super important in statistical analysis!

## Fitting a Line to Many Points: Linear Regression!
Now let's turn to another example data set. This new dataset focuses on penguins. Do you think a linear model might be a good way to model the data? Run the code below to create a scatter plot of flipper length versus body mass. 

```{r, warning = FALSE, fig.width = 5, fig.height = 4}
data(penguins)

pengscat = ggplot(penguins, aes(x = body_mass_g, 
                                y = flipper_length_mm)) + 
  geom_point(color = 'coral2', alpha = 0.7) +
  labs(x = "Body Mass (grams)",
            y = "Flipper Length (mm)",
            title = "Flipper Length by Body Mass of Penguins")
pengscat
```

Take a look at the scatterplot, does it look like most of the data fall along a straight line? If the general shape is a line, then yes, we should try to model this data with linear regression line.

```{r}
pengfit <- lm(formula = flipper_length_mm ~ body_mass_g, 
             data = penguins)
pengfit
```

Here R gives us the slope and intercept of the straight line that best fits our data. Let's graph this line together with our data using the code below. 

```{r, warning = FALSE}
pengscat + geom_abline(slope = 0.0152, intercept = 137.0396)
```

That looks like a pretty good fit, right? But how do we evaluate how good of a fit the linear regression provides? One function we can use in R is `msummary()`. This function provides an overview of the fitted model.  

```{r}
library(mosaic) # check if i can add this for a simpler summary... otherwise use just summary
msummary(pengfit)

#summary(pengfit)
```

There is a lot of information that comes out of this function, but just focus on "Multiple R-squared" for now. $R^2$ represents the percent of variation in the response y-variable that is explained by the explanatory x-variable(s). Can you find the $R^2$ for this model in the summary table? 

You probably found 0.759 on the table. This means that 75.9% of the variation in flipper length can be explained by the body mass of penguins. That is a pretty high $R^2$, especially for observational data. Therefore, we can consider this model to be quite good for modeling flipper length. 

## Categorical Data to Numerical Representations
So far we've only explored linear regression with numerical variables, but we can certainly use categorical variables in our model as well. Continuing with our penguins data, let's compare flipper length to penguin species. Remember that flipper length is a numerical variable and species is a categorical variable (with three levels: Adelie, Chinstrap, and Gentoo).

To start, let's consider one level at a time, so we can get a good sense of the relationship between species and flipper length. Below we examine Adelie penguins first. We start off by using `mutate()` again to label our species as either "Adelie" or "Not Adelie". Then, we create a side-by-side boxplot using `geom_boxplot()` again. 

```{r, warning = FALSE}
#penguins$isAdelie = ifelse(penguins$species == 'Adelie', 1, 0)

pengAdelie <- penguins %>% 
  mutate(species = case_when(species != "Adelie" ~ "Not Adelie",
                            TRUE ~ "Adelie"))

adelieplot <- ggplot(pengAdelie, aes(x = species,
                                     y = flipper_length_mm)) + 
  geom_boxplot(color = 'coral2', 
               alpha = 0.7) +
  labs(x = "Species",
       y = "Flipper Length (mm)")
adelieplot
```

We can see that Adelie penguins seem to have a shorter flipper length, on average, than non-Adelie penguins. 

Next, we create a linear model for the relationship between flipper length and whether a penguin is an Adelie penguin or not.

```{r}
amodel <- lm(formula = flipper_length_mm ~ species, 
             data = pengAdelie)
amodel
```
Then plot this best fit linear model against with our scatterplot to compare. Do you think that our linear model is a good representation of the data?

<!-- FIX THIS SECTION WITH THE REGRESSION LINE-->

As we saw before, things look a little bit different when we are dealing with categorical variables. In these types of scatterplots, we would expect the best fit model  to pass through the mean values of each level of our categorical variable (or each 'chunk' of data). See below.

```{r}
adelieplot + geom_abline(slope = 19.63, intercept = 189.95)
```
Now we do the same thing for the other two species (Chinstrap and Gentoo) and plot all three graphs side by side.

```{r, fig.width=12}
penguins$isChinstrap = ifelse(penguins$species=='Chinstrap',1,0)
penguins$isGentoo = ifelse(penguins$species=='Gentoo',1,0)

cmodel = lm(formula = flipper_length_mm~isChinstrap, data=penguins)
gmodel = lm(formula = flipper_length_mm~isGentoo, data=penguins)

speciesbase = ggplot(penguins, aes(y=flipper_length_mm)) 
aplot = speciesbase + 
  geom_point(aes(x=isAdelie), color='coral2', alpha=0.7) + 
  geom_abline(slope=amodel$coefficients[2], intercept=amodel$coefficients[1])
cplot = speciesbase + 
  geom_point(aes(x=isChinstrap), color='coral2', alpha=0.7) + 
  geom_abline(slope=cmodel$coefficients[2], intercept=cmodel$coefficients[1])
gplot = speciesbase + 
  geom_point(aes(x=isGentoo), color='coral2', alpha=0.7) + 
  geom_abline(slope=gmodel$coefficients[2], intercept=gmodel$coefficients[1])

grid.arrange(aplot, cplot, gplot, ncol=3)
```
What would happen if instead of creating linear models for each level of the categorical variable separately, we created a single linear model for flipper length versus all species types? The code below allows us to find that model in just one step.

```{r}
linmodel <- lm(flipper_length_mm ~ species, data = penguins)
linmodel
```

This model will look like $$Flipper Length = 190.103 + 5.72*(Chinstrap) + 27.133*(Gentoo)$$ where the variable Chinstrap is an indicator variable that takes on the value of 1 when the penguin is the species Chinstrap and takes on the value of 0 when the penguin is any other species. Similarly, the variable Gentoo is an indicator variable that takes on the value of 1 when the penguin is the species Gentoo and takes on the value of 0 when the penguin is any other species.

So...what happened to the Adelie penguins in our model?!?! Notice the variable for Adelie is missing in our model. It actually got absorbed by the y-intercept! We only need n-1 variables to represent n levels of a categorical variable because we have this y-intercept. When a penguin is of the species Adelie and we apply this linear model, the values for Gentoo and Chinstrap are both equal to 0, since the penguin is an Adelie. Thus our model would predict a flipper length of 190.103 for all Adelie penguins.

What is the flipper length predicted by our model for a Chinstrap penguin? What about a Gentoo penguin?

```{r}
peng_encoded = penguins %>% mutate(value = 1) %>% spread(species, value, fill = 0 )
head(peng_encoded)
```

## Using Linear Regression with the Federal Criminal Sentencing Data

Now that we've explored the basics of linear regression using both numerical and categorical variables, let's return to the goal of this project: to explore patterns in federal criminal sentencing data from 2006-2020. We will show how to create relevant linear models for this case study, and discuss the results of these models in the following section. We won't dive into the specifics of why, based on the EDA, we will include certain variables over others in the linear regression. Rather, we will simply replicate the results of the paper mentioned at the beginning of this case study. For more information on how the researchers of that paper arrived their final models, please read the paper.

### Baseline Model

Let's try a model of `sentence_length` based on the variable `race`, using the same methods we used above.
```{r}
baselinemod <- lm(sentence_length ~ race,
                  data = us_sent)
msummary(baselinemod)
```
<!-- How should we do confidence intervals in this case?

We can also figure out confidence intervals using the following code:

```{r}
library(DescTools)

us_sent %>% 
  drop_na(race) %>%
  count(race) %>%
  data.frame(MultinomCI(.$n, conf.level = 0.95))
```

-->

### District I Model
The first model includes defendant demographics, sentencing year, whether or not there was a guilty plea, relevant cell on the U.S. sentencing grid, whether there was a mandatory minimum sentence, presence of government-sponsored downward departures, and the interaction of judicial district and defendant race. An interaction term is employed when the one variable impacts another variable in some wayFor our dataset this includes the following variables: `age`, `sex`, and `educ`, `year`, `guilty_plea`, `grid_cell`, `mandatory_min`, `gov_departures`, and the interaction of `district` and `race`. In this model, all terms except for the interaction derive from nationwide data. For example, this model assumes that all districts, on average, apply the U.S. sentencing grid in the same way. The code for formulating this linear regression model would follow the same format as the other linear models we've made:

```{r, eval = FALSE}
lm1 <- lm(sentence_length ~ age + sex + educ + year + guilty_plea + grid_cell + mandatory_min + 
            gov_departures + district:race,
          data = us_sent)
```

### District II Model
The second model relies on the same variables as in the District I Model plus `race` on its own. However, a regression is performed within each district, rather than across all districts represented in the data set. This model allows for variations in grid application, defendant demographics, and more. To perform this form of regression in R, you would create a subset of `us_sent` for each district, and then employ the same code as above but with each smaller data set. 

